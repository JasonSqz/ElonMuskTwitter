[["index.html", "Analyzing emotions towards Elon Musk on Twitter Chapter 1 Introduction", " Analyzing emotions towards Elon Musk on Twitter Anni Chen, Zehui Wu, Senqi Zhang 2021-12-08 Chapter 1 Introduction "],["data-sources.html", "Chapter 2 Data sources 2.1 Dataset overview 2.2 Limitations", " Chapter 2 Data sources Our data was collected from Twitter using the Tweepy API. Since we are interested in Elon Musk. We chose the keyword set of ‚ÄúElon Musk‚Äù,‚Äúelonmusk‚Äù,‚Äú@elonmusk‚Äù,‚Äútesla‚Äù,‚ÄúTesla‚Äù. From October 28th to Novemeber 5th, we collected data 9 days in a row. For the purpose of this study, we choose to use data from October 28th to Novemeber 3rd, which is a 7-day period. During the period when our program was running, we capture every real-time tweets that contain any of the above keywords. The data is collected in the form of a tweet object, and it contains not only the text of the tweet, but other valuable information worth exploring. 2.1 Dataset overview After combining our dataset, we observed that a total of 471,996 tweets were collected. For a single tweet object, it contains 35 features. Below is a table of the features. all variable of the raw data created_at id id_str text display_text_range source truncated in_reply_to_status_id in_reply_to_status_id_str in_reply_to_user_id in_reply_to_user_id_str in_reply_to_screen_name user geo coordinates place contributors is_quote_status extended_tweet quote_count reply_count retweet_count favorite_count entities favorited retweeted filter_level lang timestamp_ms quoted_status_id quoted_status_id_str quoted_status quoted_status_permalink possibly_sensitive extended_entities One thing worth mentioning is that some of features are actually dictionaries. For example, in User, there are actually 40 more sub features including username,location,description, etc.. Counting those sub features, there are over 150 features in a tweet object and we have to choose carefully from them. Details of selected features and descriptions will be elaborated in the data transformation section. 2.2 Limitations Since we do not have the resource to a cloud server, we were not able to run the program 24 hours every day in the period. Moreover, the start and end time for each date is different. We will need to find a overlapping period when conducting analysis relating to time. Another limitation is about a feature called ‚Äúconversation_id‚Äù. It builds up a conversation by capturing replies to a specific tweet. However, it has to be configured during the data collection phase and our dataset does not have it. We will try some other methods to construct a conversation. "],["data-transformation.html", "Chapter 3 Data transformation 3.1 Feature Selection 3.2 Tokenization 3.3 Sentiment 3.4 Summary of added features", " Chapter 3 Data transformation 3.1 Feature Selection There are 35 features in the data set and more sub-features within a feature. After careful consideration, we chose 7 features to construct our data set for analysis. The following is a table describing all selected variables. Features Description created_at creation time of tweet id_str tweet id text truncated tweet of length&lt;140 user_id user id followers_count number of followers of user location location of tweet full_text full text of tweet Of the 7 features, user_id and follower_count are sub-features from user. They captures the id and the number of followers of a user respectively. location is a sub-feature from place. It is a user-identified location, which they can put anything on it. full_text is a sub-feature from extended tweet. It captures tweets that has over 140 characters. We did not keep the feature geo as we found out that it has a missing percentage of almost 100 percent. It will show up in the missing value section to support out decision. After selecting our basic features, we notice that texts are stored in two different features. If the tweet is longer than 140 characters, the tweet would be truncated and put into text. The original tweet would be put into full_text.To make our analysis more convenient, we add a new variable called original_text to store the tweets 3.2 Tokenization Next, we conduct tokenization. This a process that will help us with other natural langauge processing analysis. Since there are some special characters like hashtag and usernames that might otherwise be stripped away using other tokenizer, we use the specific tokenize_tweets() function from the tokenizers library. Below is a demonstration for how it tokenizes one of the tweets. ## [1] &quot;@AGlobalCitizen @AlexanderBruyns @PPathole @elonmusk @SpaceX We would also somehow need to create some sort of artificial magnetic field around the entire planet to make sure that any biomass we take there isn&#39;t immediately destroyed by cosmic radiation&quot; ## [[1]] ## [1] &quot;@AGlobalCitizen&quot; &quot;@AlexanderBruyns&quot; &quot;@PPathole&quot; &quot;@elonmusk&quot; ## [5] &quot;@SpaceX&quot; &quot;we&quot; &quot;would&quot; &quot;also&quot; ## [9] &quot;somehow&quot; &quot;need&quot; &quot;to&quot; &quot;create&quot; ## [13] &quot;some&quot; &quot;sort&quot; &quot;of&quot; &quot;artificial&quot; ## [17] &quot;magnetic&quot; &quot;field&quot; &quot;around&quot; &quot;the&quot; ## [21] &quot;entire&quot; &quot;planet&quot; &quot;to&quot; &quot;make&quot; ## [25] &quot;sure&quot; &quot;that&quot; &quot;any&quot; &quot;biomass&quot; ## [29] &quot;we&quot; &quot;take&quot; &quot;there&quot; &quot;isnt&quot; ## [33] &quot;immediately&quot; &quot;destroyed&quot; &quot;by&quot; &quot;cosmic&quot; ## [37] &quot;radiation&quot; We then add word_tokens as a feature to our dataset. 3.3 Sentiment Tweets have sentiments and here we try to classify a tweet as positive, negative or netural. Before extracting sentiments from the tweets, we need to firstly clean the text such that it does not contain any special characters such as hashtags, ‚Äú\",‚Äù@\", website links, etc. Some special character might affect the accuracy of the sentiment score. Below is a comparison between original text and the cleaned text. Typically ‚Äú@‚Äù and website links are removed. After cleaning all the tweets, they were stored in a new feature cleaned_text in our dataset. ## [1] &quot;@elonmusk @ElemonGame Great ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è @Corsair will help @Elemongame the first blockchain game ever owns hundred millions users. This is far beyond my imagination. To the Moon and Mars. @ElemonGame @CORSAIR #Elemon #Corsair love ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏èüöÄüöÄüöÄ https://t.co/iakL1e8brw&quot; ## [1] &quot;elonmusk ElemonGame Great ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è Corsair will help Elemongame the first blockchain game ever owns hundred millions users. This is far beyond my imagination. To the Moon and Mars. ElemonGame CORSAIR Elemon Corsair love ‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏èüöÄüöÄüöÄ &quot; Now, we determine the sentiment score for each tweet using library ‚Äúsyuzhet‚Äù, which is a custom sentiment dictionary developed in the Nebraska Literary Lab.The sentiment scores are stored in a new feature sentiment_score. We then classify each tweet into three categories: positive(score&gt;0), neutral(socre=0), and negative(score&lt;0). We add sentiment as feature into our dataset to capture the sentiment category of a tweet. 3.4 Summary of added features We added 5 new features into the dataset after some processing, adding to a total of 12 features. Those new features will help us better conduct analysis and visualizations. Below is a table describing all added features. variables descriptions original_text original text word_tokens a list of word tokens cleaned_text text after removing special characters sentiment_score sentiment scores sentiment sentiment of tweet: positive, neutral, or negative "],["missing-values.html", "Chapter 4 Missing values 4.1 Twitter Dataset", " Chapter 4 Missing values 4.1 Twitter Dataset We conducted analysis on the missing values before forming our final dataset. Of all the features, we first choose 8 features to consider. 7 out of the 8 features are introduced in the previous section, and the only feature that we left out is geo. The missing value graph will support our decision. Another thing worth mentioning is that all the added features will not be presented. Since they are dervied from text, and they will not contain any missing value. Here we use the whole raw data set containing tweets from October 28th to Novemeber 3rd. There are a total of 471,996 tweets. The following is a missing value graph of our raw dataset. We modified the feature names a bit to make sure the graph is clean. As shown in the graph, the geo feature has the most missing values that is close to 100%. From the documentation, geo is the tweet location that is tagged by the user. It indicates that Twitter users seldom specify a location for tweets. Because of the high missing percentage, we choose to remove the feature. Another largely missing data column is location with 5241 valid values. location is a self-identified location by the user, and is has the format like ‚ÄòManhattan, NY‚Äô. We think that we will be able to derive some geological pattern based on the limited data we have. The missing value in text is interesting. text can only holds up to 140 character and if a tweet exceeds the limit, full_text is the feature that will capture the complete contents. We see that around half of the users write short text when post tweets related to Elon Musk. Looking at the missing patterns. More than a half of the tweets are missing geo and location, and around 46% of the tweets have additional missing value in full_text. The other three missing patterns are trivial. We do notice that there is a missing pattern that has missing value in every feature. We found out that it was a empty row in our dataset and we removed that row manually. "],["results.html", "Chapter 5 Results", " Chapter 5 Results "],["interactive-component.html", "Chapter 6 Interactive component", " Chapter 6 Interactive component "],["conclusion.html", "Chapter 7 Conclusion", " Chapter 7 Conclusion "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
