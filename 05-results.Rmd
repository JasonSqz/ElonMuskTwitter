# Results
```{r, message=FALSE, warning=FALSE}
library(jsonlite)
library(tidyverse)
library(dplyr)
library(knitr)
library(tokenizers)
library(syuzhet)
library(ggplot2)
library(tm)
library(tidytext)
library(wordcloud)
library(lubridate)
library(scales)
library(usmap)
```

```{r}
#Read in the dataset
df <- read.csv("data/EM_df.csv",encoding="latin1")
df <- df %>%
  select(-c(X,geo.type,geo.coordinates)) 

```

```{r}
#Reformat timestamp
df <- df %>%
  mutate(created_at = as.character(strptime(created_at,"%a %b %d %H:%M:%S %z %Y", tz = "EST"))) %>%
  mutate(date = as.Date(created_at))

```

```{r}
#Calculate the duration of collecting period for each day
df_1028 <- df %>%
  filter(date=="2021-10-28")

dt1028 <- difftime(df_1028$created_at[nrow(df_1028)], df_1028$created_at[1], unit="hours")

df_1029 <- df %>%
  filter(date=="2021-10-29")

dt1029 <- difftime(df_1029$created_at[nrow(df_1029)], df_1029$created_at[1], unit="hours")

df_1030 <- df %>%
  filter(date=="2021-10-30")

dt1030 <- difftime(df_1030$created_at[nrow(df_1030)], df_1030$created_at[1], unit="hours")

df_1031 <- df %>%
  filter(date=="2021-10-31")

dt1031 <- difftime(df_1031$created_at[nrow(df_1031)], df_1031$created_at[1], unit="hours")

df_1101 <- df %>%
  filter(date=="2021-11-01")

dt1101 <- difftime(df_1101$created_at[nrow(df_1101)], df_1101$created_at[1], unit="hours")

df_1102 <- df %>%
  filter(date=="2021-11-02")

dt1102 <- difftime(df_1102$created_at[nrow(df_1102)], df_1102$created_at[1], unit="hours")

df_1103 <- df %>%
  filter(date=="2021-11-03")

dt1103 <- difftime(df_1103$created_at[nrow(df_1103)], df_1103$created_at[1], unit="hours")

tn <- c(dt1028,dt1029,dt1030,dt1031,dt1101,dt1102,dt1103)
tn <- as.numeric(tn)
```

## Elon Musk

Elon Musk is very active on twitter and anything he tweets can be very influential and brings a lot of controversies. For example, his tweet about the crypto-currency "dogecoin" caused "dogecoin" price to spike. Since Twitter users have many discussions surrounding him, our analysis aims to visualize those discussions. 

The period of which we collected our data was very interesting. It was shortly after Tesla flied in stock market and Elon Musk became the richest man in the world. He was also asked by the United Nation to donate 4% of his growing wealth to solve a hunger crisis. Just shortly after that, on Nov. 1st, he posted a Chinese poem "七步诗" titled as "Humankind".


![](data/poem.png)

## Volume of tweets

We first carried out analysis on the number of tweets that we collected each day. Due to some technical issues, the durations we collected tweets each day are different. They were about 10-12 hours each day, with one excepetion on Oct 29. On that day, we collected tweets for about 5.5 hours. In order to present a consistent results through the period, we standardize the tweets by dividing the total collected hour and multiplying by 24. The following graph presents expected number of tweets each day.

```{r}
time_df <- df %>% 
  group_by(date) %>%
  count() %>%
  filter(is.na(date) == FALSE) %>%
  add_column(period=tn) %>%
  mutate(standardized= (n*24)/period) %>%
  ungroup

ggplot(time_df, aes(date,standardized)) +
    geom_line(color="lightblue3") + 
    scale_x_date(date_breaks="1 day") +
    ggtitle("Expected number of tweets each day") +
    xlab("Date") +
    ylab("Standardized number of tweets") +
    theme_bw()
```

From the graph, we see that the volume each day can be as low as 70 thousands and as high as 250 thousands. There were two peaks on Oct. 29th and Nov 1st. For the first peak, we corresponded it to the drastic increase in Tesla's stock price. For the second peak, we related to the conversation between an United Nation offical and Elon Musk. We notice from below that this conversation has 337.5 thousand likes, showing its tremendous volume of attention. Observing these patterns, we conclude that controversial events associated with Elon Musk will cause an increase in discussion on Twitter.  

<center>
![](data/UN_EM.png){width=50%}
</center>

## Geological Distribution

Even though we have limited data on geological information of users, we still want to make use of them to explore some patterns.
In total we have 5293 tweets that contain a user geological information. The location is not limited to the United States, and we conducted filtering for each states.

```{r}
geo_df <- df %>%
  filter(!is.na(location))
```

```{r}
state_list <- c("Alabama","AL","Alaska","AK",	"Arizona","AZ","Arkansas","AR","California","CA","Colorado","CO",
"Connecticut","CT","Delaware","DE","Florida","FL","Georgia","GA","Hawaii","HI","Idaho","ID","Illinois","IL",
"Indiana","IN","Iowa","IA","Kansas","KS","Kentucky","KY","Louisiana","LA",	"Maine","ME","Maryland","MD",	
"Massachusetts","MA",	"Michigan","MI",	"Minnesota","MN",	"Mississippi","MS",	"Missouri","MO","Montana","MT",	
"Nebraska","NE",	"Nevada","NV","New Hampshire","NH","New Jersey","NJ","New Mexico",	"NM","New York",	"NY",
"North Carolina",	"NC","North Dakota",	"ND","Ohio", "OH",	"Oklahoma",	"OK",	"Oregon",	"OR",	"Pennsylvania",	"PA",	
"Rhode Island",	"RI","South Carolina",	"SC",	"South Dakota",	"SD","Tennessee",	"TN",	"Texas","TX","Utah","UT",
"Vermont","VT","Virginia","VA","Washington","WA","West Virginia","WV","Wisconsin","WI","Wyoming","WY")
```

```{r}
for (i in 1:nrow(geo_df)){
  loc <- geo_df$location[i]
  loc_split1 <- str_split(loc,", ")[[1]][1]
  loc_split2 <- str_split(loc,", ")[[1]][2]
  
  if(loc_split2 %in% state_list == TRUE){
    geo_df$location[i] = loc_split2
  } else {
    geo_df$location[i] = NA
  }
}
```

```{r}
geo_df_r <- geo_df %>%
  filter(!is.na(location)) %>%
  group_by(location) %>%
  count()

colnames(geo_df_r)[1] <- "state"
```

```{r}
plot_usmap(data=geo_df_r,values="n") + 
  scale_fill_continuous(low = "white", high = "deepskyblue", name = "Tweet posted") +
  labs(title = "United States", subtitle = "Number of tweets posted by states") +
  theme(panel.background=element_blank(),legend.position = "right")
    
```

From the state graph, we can see that users mainly locate in four states. They are California, Texas, Florida, and New York, with 503, 336, 248, and 187 users respectively. Even though we have a fairly small sample, it still seems that users in more economically developed state tend to discuss more related to Elon Musk.

## Tweet Contents

### Tweet Sentiment
Now, our analysis dive into the tweet-level. We are concerned with the contents of the tweets. Even though it is impossible for us to examine tweet by tweet, we use powerful Natural Language Processing techniques to explore patterns. 

We try to identify the emotion of tweets using sentiment analysis. Each tweet will be tagged a sentiment score.

```{r}
# fix sentiment column
for (i in 1:length(df$sentiment)){
  if(!(df$sentiment[i] %in% c("neutral","negative","positive"))){
    if(df$sentiment[i]>0){
      df$sentiment[i]="positive"
    }
    if(df$sentiment[i]==0){
      df$sentiment[i]="neutral"
    }
    if(df$sentiment[i]<0){
      df$sentiment[i]="negative"
    }
  }
}
```



```{r}
# create corpus
df$cleaned_text = tolower(df$cleaned_text)
text <- removeWords(df$cleaned_text, words = stopwords(kind = "en"))
text <- tibble(txt=text)
tweets_words <-  text %>%
 unnest_tokens(word, txt)
words <- tweets_words %>% count(word, sort=TRUE)
```

```{r}
#word frequency histogram
words %>%
  top_n(15) %>%
ggplot()+
  geom_bar(aes(x=word,y=n),stat="identity")+
  ggtitle("top 15 words of all tweets") +
  xlab("count")+
  ylab("word")
```

```{r}
#word cloud
wordcloud(words = words$word, freq = words$n, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35, colors=brewer.pal(8, "Dark2"))
```

```{r}
#length of tweet histogram
df <- df %>% mutate(length =nchar(original_text))
ggplot(df)+
  geom_histogram(aes(length),stat="count")+
  xlim(0,1000)+
  ggtitle("Distribution of length of all tweets")
```


```{r}
#sentiment frequency histogram
df %>%
ggplot(aes(x=as.double(sentiment_score))) + 
  geom_histogram(binwidth = 1, fill = "steelblue")+ 
  ylab("Frequency") + 
  xlab("sentiment score") +
  ggtitle("Distribution of Sentiment scores of all tweets")+
  scale_x_continuous(limits=c(-5,5))
```

```{r}
df %>%
ggplot()+
  geom_bar(aes(x=as.factor(sentiment),fill = sentiment),stat = "count")+
  ggtitle("Sentiments of all tweets")
```

### Monitoring Topics

After the exploration of sentiments, we want to further know what specific topics users favor to discuss. To to this, we apply topic modeling(LDA) to find underlying topics. The [**link**](lda5.html) leads to an interactive LDA result. 



