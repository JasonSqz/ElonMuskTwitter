# Data sources

Our data was collected from Twitter using the Tweepy API. Since we are interested in Elon Musk, we chose the keyword set of `Elon Musk`, `elonmusk`, `@elonmusk`, `tesla`, `Tesla`. From October 28th to November 3rd, we collected data 7 days in a row. During the period when our program was running, we captured every real-time tweets that contain any of the above keywords. The data was collected in the form of a tweet object, and it contained not only the text of the tweet, but other valuable information worth exploring. 

## Dataset overview

After combining our dataset, we observed that a total of 471,996 tweets were collected. For a single tweet object, it contains 36 features. Below is a table of the features. 

```{r}
library(dplyr)
library(jsonlite)
```

```{r,message=FALSE, warning=FALSE,echo=FALSE, results='hide'}
json_data <- stream_in(file("data/demo_df.json"))
```

```{r}
col_names_raw <- colnames(json_data) %>% as.data.frame()
knitr::kable(cbind(col_names_raw[1:18,],col_names_raw[19:34,]),
             col.names = c("all variable of the raw data",""),
             caption = "All Feature names")
```

One thing worth mentioning is that some of features are actually dictionaries. For example, in `User`, there are actually 40 more sub features including `username`,`location`,`description`, etc.. Counting those sub features, there are over 150 features in a tweet object and we have to choose carefully from them. Details of selected features and descriptions will be elaborated in the data transformation section.  

## Limitations

Since we do not have the resource to a cloud server, we were not able to run the program 24 hours every day in the period. Moreover, the start and end time for each date is different. We will need to find a overlapping period when conducting analysis relating to time. Another limitation is about a feature called "conversation_id". It builds up a conversation by capturing replies to a specific tweet. However, it has to be configured during the data collection phase and our dataset does not have it. We will try some other methods to construct a conversation. 




